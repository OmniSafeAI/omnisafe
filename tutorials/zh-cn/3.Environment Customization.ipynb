{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OmniSafe Tutorial - Environment Customization\n",
    "\n",
    "OmniSafe: https://github.com/PKU-Alignment/omnisafe\n",
    "\n",
    "Documentation: https://omnisafe.readthedocs.io/en/latest/\n",
    "\n",
    "Safety-Gymnasium: https://www.safety-gymnasium.com/\n",
    "\n",
    "[Safety-Gymnasium](https://www.safety-gymnasium.com/) is a highly scalable and customizable Safe Reinforcement Learning library, aiming to deliver a good view of benchmarking Safe Reinforcement Learning (Safe RL) algorithms and a more standardized setting of environments. \n",
    "\n",
    "## 引言\n",
    "\n",
    "在本节当中，我们将为您介绍如何在OmniSaef中自定义环境。我们付出了很大的努力将环境接口与OmniSafe的内部逻辑解耦合。您只需要基于我们提供的最小模版作相应改动，即可享受全套的参数定制、算法训练以及结果保存功能。\n",
    "\n",
    "具体而言，我们将从以下两个部分出发：\n",
    "\n",
    "- **从OmniSafe的最简环境定制化模版出发** 我们提供了一个用于定制化环境的最简单模版。通过该模版，你将了解：\n",
    "  - 定制化环境注册。\n",
    "  - 环境创建参数指定。\n",
    "  - 环境特定信息记录。\n",
    "\n",
    "## 快速安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过pip安装（如果您已经安装，请忽略此段代码）\n",
    "!pip install omnisafe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过源代码安装（如果您已经安装，请忽略此段代码）\n",
    "## 克隆仓库\n",
    "!git clone https://github.com/PKU-Alignment/omnisafe\n",
    "%cd omnisafe\n",
    "\n",
    "## 完成安装\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定制化环境最简模版\n",
    "OmniSafe的定制化环境可以仅通过单个文件实现。我们将为您介绍一个最简的定制化环境模版，它将作为您入门的起点。\n",
    "\n",
    "### 定制化环境设计\n",
    "我们将在此细致地介绍一个简易随机环境的定制过程。如果您是强化学习领域的专家或有经验的研究者，可以跳过该模块至[定制化环境嵌入](#定制化环境嵌入)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiayi/miniconda3/envs/omnisafe/lib/python3.8/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的包\n",
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "import omnisafe\n",
    "from typing import Any, ClassVar\n",
    "\n",
    "import torch\n",
    "from gymnasium import spaces\n",
    "\n",
    "from omnisafe.envs.core import CMDP, env_register, remove_from_register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义环境类\n",
    "class ExampleEnv(CMDP):\n",
    "    _support_envs: ClassVar[list[str]] = ['Example-v0']  # 支持的任务名称\n",
    "\n",
    "    need_auto_reset_wrapper = True  # 是否需要 `AutoReset` Wrapper\n",
    "    need_time_limit_wrapper = True  # 是否需要 `TimeLimit` Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您需要关注上面这段代码的如下细节：\n",
    "\n",
    "- **任务名称定义** 在 `_support_envs` 中定义环境支持的任务名称。\n",
    "- **Wrapper配置** 通过设定 `need_auto_reset_wrapper` 和 `need_time_limit_wrapper` 来定义自动重置和限制时间。\n",
    "- **并行环境数量** 如果您的环境支持向量化并行，请通过 `_num_envs` 参数进行设定。\n",
    "\n",
    "`AutoReset` Wrapper与 `TimeLimit` Wrapper 的更多信息将在本教程的剩余部分介绍。在此处您只需了解可以通过此接口进行开关即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleEnv(CMDP):\n",
    "    _support_envs: ClassVar[list[str]] = ['Example-v0', 'Example-v1']  # 支持的任务名称\n",
    "\n",
    "    need_auto_reset_wrapper = True  # 是否需要 `AutoReset` Wrapper\n",
    "    need_time_limit_wrapper = True  # 是否需要 `TimeLimit` Wrapper\n",
    "\n",
    "    def __init__(self, env_id: str, **kwargs) -> None:\n",
    "        self._count = 0\n",
    "        self._num_envs = 1\n",
    "        self._observation_space = spaces.Box(low=-1.0, high=1.0, shape=(3,))\n",
    "        self._action_space = spaces.Box(low=-1.0, high=1.0, shape=(2,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成 `__init__` 函数定义。此处需要指定环境的动作空间与观测空间。你也可以根据具体的任务来定义。例如在本例子中，您可以尝试：\n",
    "```python\n",
    "if env_id == 'Example-v0':\n",
    "    self._observation_space = spaces.Box(low=-1.0, high=1.0, shape=(3,))\n",
    "    self._action_space = spaces.Box(low=-1.0, high=1.0, shape=(2,))\n",
    "elif env_id == 'Example-v1':\n",
    "    self._observation_space = spaces.Box(low=-1.0, high=1.0, shape=(4,))\n",
    "    self._action_space = spaces.Box(low=-1.0, high=1.0, shape=(3,))\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "```\n",
    "**请注意，在定制化时请勿修改变量名 `self._observation_space` 以及 `self._action_space`。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成环境初始化相关函数的定义。我们认为 `reset` 和 `set_seed` 是环境初始化的必要函数。其中 `reset` 重置环境状态与计步器。而 `set_seed` 通过设定随机种子确保实验的可复现性。实现参考如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleEnv(CMDP):\n",
    "    _support_envs: ClassVar[list[str]] = ['Example-v0', 'Example-v1']  # 支持的任务名称\n",
    "\n",
    "    need_auto_reset_wrapper = True  # 是否需要 `AutoReset` Wrapper\n",
    "    need_time_limit_wrapper = True  # 是否需要 `TimeLimit` Wrapper\n",
    "\n",
    "    def __init__(self, env_id: str, **kwargs) -> None:\n",
    "        self._count = 0\n",
    "        self._num_envs = 1\n",
    "        self._observation_space = spaces.Box(low=-1.0, high=1.0, shape=(3,))\n",
    "        self._action_space = spaces.Box(low=-1.0, high=1.0, shape=(2,))\n",
    "\n",
    "    def set_seed(self, seed: int) -> None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        seed: int | None = None,\n",
    "        options: dict[str, Any] | None = None,\n",
    "    ) -> tuple[torch.Tensor, dict]:\n",
    "        if seed is not None:\n",
    "            self.set_seed(seed)\n",
    "        obs = torch.as_tensor(self._observation_space.sample())\n",
    "        self._count = 0\n",
    "        return obs, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成功能性函数的定义。`sample_action` 函数用于从环境中随机采样动作；`render` 函数用于渲染环境；`close` 函数用于训练结束后的清理。其中 `sample_action` 是 `off-policy` 和 `offline` 算法必须使用的函数，`render` 和 `close` 是可选的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleEnv(CMDP):\n",
    "    _support_envs: ClassVar[list[str]] = ['Example-v0', 'Example-v1']  # 支持的任务名称\n",
    "\n",
    "    need_auto_reset_wrapper = True  # 是否需要 `AutoReset` Wrapper\n",
    "    need_time_limit_wrapper = True  # 是否需要 `TimeLimit` Wrapper\n",
    "\n",
    "    def __init__(self, env_id: str, **kwargs) -> None:\n",
    "        self._count = 0\n",
    "        self._num_envs = 1\n",
    "        self._observation_space = spaces.Box(low=-1.0, high=1.0, shape=(3,))\n",
    "        self._action_space = spaces.Box(low=-1.0, high=1.0, shape=(2,))\n",
    "\n",
    "    def set_seed(self, seed: int) -> None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        seed: int | None = None,\n",
    "        options: dict[str, Any] | None = None,\n",
    "    ) -> tuple[torch.Tensor, dict]:\n",
    "        if seed is not None:\n",
    "            self.set_seed(seed)\n",
    "        obs = torch.as_tensor(self._observation_space.sample())\n",
    "        self._count = 0\n",
    "        return obs, {}\n",
    "\n",
    "    def sample_action(self) -> torch.Tensor:\n",
    "        return torch.as_tensor(self._action_space.sample())\n",
    "\n",
    "    def render(self) -> Any:\n",
    "        pass\n",
    "\n",
    "    def close(self) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成 `step` 函数定义。此处是您定制化环境的核心交互逻辑。您只需按照本例中的数据输入与输出格式进行调整即可。您也可以直接将本例中的随机交互动态更改为您的环境动态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleEnv(CMDP):\n",
    "    _support_envs: ClassVar[list[str]] = ['Example-v0', 'Example-v1']  # 支持的任务名称\n",
    "    metadata: ClassVar[dict[str, int]] = {}\n",
    "\n",
    "    need_auto_reset_wrapper = True  # 是否需要 `AutoReset` Wrapper\n",
    "    need_time_limit_wrapper = True  # 是否需要 `TimeLimit` Wrapper\n",
    "\n",
    "    def __init__(self, env_id: str, **kwargs) -> None:\n",
    "        self._count = 0\n",
    "        self._num_envs = 1\n",
    "        self._observation_space = spaces.Box(low=-1.0, high=1.0, shape=(3,))\n",
    "        self._action_space = spaces.Box(low=-1.0, high=1.0, shape=(2,))\n",
    "\n",
    "    def set_seed(self, seed: int) -> None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        seed: int | None = None,\n",
    "        options: dict[str, Any] | None = None,\n",
    "    ) -> tuple[torch.Tensor, dict]:\n",
    "        if seed is not None:\n",
    "            self.set_seed(seed)\n",
    "        obs = torch.as_tensor(self._observation_space.sample())\n",
    "        self._count = 0\n",
    "        return obs, {}\n",
    "\n",
    "    def sample_action(self) -> torch.Tensor:\n",
    "        return torch.as_tensor(self._action_space.sample())\n",
    "\n",
    "    def render(self) -> Any:\n",
    "        pass\n",
    "\n",
    "    def close(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        action: torch.Tensor,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, dict]:\n",
    "        self._count += 1\n",
    "        obs = torch.as_tensor(self._observation_space.sample())\n",
    "        reward = 10000 * torch.as_tensor(random.random())\n",
    "        cost = 10000 * torch.as_tensor(random.random())\n",
    "        terminated = torch.as_tensor(random.random() > 0.9)\n",
    "        truncated = torch.as_tensor(self._count > 10)\n",
    "        return obs, reward, cost, terminated, truncated, {'final_observation': obs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们试着运行该环境10个时间步，观察交互信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "obs: tensor([-0.1833,  0.3815,  0.4920])\n",
      "reward: 8444.21875\n",
      "cost: 7579.5439453125\n",
      "terminated: False\n",
      "truncated: False\n",
      "********************\n",
      "--------------------\n",
      "obs: tensor([ 0.7797, -0.6080, -0.4862])\n",
      "reward: 2589.167236328125\n",
      "cost: 5112.7470703125\n",
      "terminated: False\n",
      "truncated: False\n",
      "********************\n",
      "--------------------\n",
      "obs: tensor([-0.5981, -0.4453, -0.0885])\n",
      "reward: 7837.98583984375\n",
      "cost: 3033.127197265625\n",
      "terminated: False\n",
      "truncated: False\n",
      "********************\n",
      "--------------------\n",
      "obs: tensor([-0.1391, -0.2324,  0.0532])\n",
      "reward: 5833.8203125\n",
      "cost: 9081.12890625\n",
      "terminated: False\n",
      "truncated: False\n",
      "********************\n",
      "--------------------\n",
      "obs: tensor([ 0.8725,  0.7580, -0.3909])\n",
      "reward: 2818.37841796875\n",
      "cost: 7558.0419921875\n",
      "terminated: False\n",
      "truncated: False\n",
      "********************\n",
      "--------------------\n",
      "obs: tensor([-0.5401, -0.5251,  0.2033])\n",
      "reward: 2505.0634765625\n",
      "cost: 9097.4619140625\n",
      "terminated: True\n",
      "truncated: False\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "env = ExampleEnv(env_id='Example-v0')\n",
    "env.reset(seed=0)\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, cost, terminated, truncated, info = env.step(action)\n",
    "    print('-' * 20)\n",
    "    print(f'obs: {obs}')\n",
    "    print(f'reward: {reward}')\n",
    "    print(f'cost: {cost}')\n",
    "    print(f'terminated: {terminated}')\n",
    "    print(f'truncated: {truncated}')\n",
    "    print('*' * 20)\n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恭喜你！已经成功完成了基础的环境定义，接下来，我们将介绍如何将该环境注册入OmniSafe中，并实现环境参数传递、交互信息记录、算法训练以及结果保存等步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定制化环境嵌入\n",
    "\n",
    "### 快速训练\n",
    "\n",
    "首先，我们需要将这个环境注册入OmniSafe支持的环境列表中。得益于OmniSafe精心设计的注册机制，我们只需一个装饰器即可实现环境的注册。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@env_register\n",
    "class ExampleEnv(ExampleEnv):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果您后续需要对环境作修改，直接重新注册将会报错。这是因为我们避免产生重名环境冲突。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@env_register\n",
    "class CustomExampleEnv(ExampleEnv):\n",
    "    example_configs = 1\n",
    "\n",
    "\n",
    "env = CustomExampleEnv('Example-v0')\n",
    "env.example_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这时，您需要先对环境手动取消注册。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@remove_from_register\n",
    "class CustomExampleEnv(ExampleEnv):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后，您就可以重新注册该环境了。在本教程中，我们会同时嵌套 `env_register` 和 `remove_from_register` 装饰器，这是为了避免环境重复注册造成报错，即确保该环境只被注册一次，以便用户在阅读本教程时多次修改与运行代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomExampleEnv has not been registered yet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@env_register\n",
    "@remove_from_register\n",
    "class CustomExampleEnv(ExampleEnv):\n",
    "    example_configs = 2\n",
    "\n",
    "\n",
    "env = CustomExampleEnv('Example-v0')\n",
    "env.example_configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随后，你可以使用OmniSafe中的算法来训练这个自定义环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PPOLag.yaml from /home/jiayi/omnisafe_zjy/omnisafe/utils/../configs/on-policy/PPOLag.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiayi/miniconda3/envs/omnisafe/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Logging data to .</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">/runs/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">PPOLag-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">{Example-v0}</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">/seed-000-2024-03-26-23-04-34/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">progress.csv</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mLogging data to .\u001b[0m\u001b[1;35m/runs/\u001b[0m\u001b[1;95mPPOLag-\u001b[0m\u001b[1;36m{\u001b[0m\u001b[1;36mExample-v0\u001b[0m\u001b[1;36m}\u001b[0m\u001b[1;35m/seed-000-2024-03-26-23-04-34/\u001b[0m\u001b[1;95mprogress.csv\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Save with config in config.json</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mSave with config in config.json\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">INFO: Start training</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mINFO: Start training\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/jiayi/miniconda3/envs/omnisafe/lib/python3.8/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/jiayi/miniconda3/envs/omnisafe/lib/python3.8/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Metrics                        </span>┃<span style=\"font-weight: bold\"> Value                  </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Metrics/EpRet                  │ 28129.7109375          │\n",
       "│ Metrics/EpCost                 │ 34804.609375           │\n",
       "│ Metrics/EpLen                  │ 5.0                    │\n",
       "│ Train/Epoch                    │ 0.0                    │\n",
       "│ Train/Entropy                  │ 1.4189385175704956     │\n",
       "│ Train/KL                       │ 0.0001909744751174003  │\n",
       "│ Train/StopIter                 │ 1.0                    │\n",
       "│ Train/PolicyRatio/Mean         │ 1.0                    │\n",
       "│ Train/PolicyRatio/Min          │ 1.0                    │\n",
       "│ Train/PolicyRatio/Max          │ 1.0                    │\n",
       "│ Train/PolicyRatio/Std          │ 0.0                    │\n",
       "│ Train/LR                       │ 0.00019999999494757503 │\n",
       "│ Train/PolicyStd                │ 1.0                    │\n",
       "│ TotalEnvSteps                  │ 10.0                   │\n",
       "│ Loss/Loss_pi                   │ -7.62939453125e-06     │\n",
       "│ Loss/Loss_pi/Delta             │ -7.62939453125e-06     │\n",
       "│ Value/Adv                      │ 1.1920928955078125e-07 │\n",
       "│ Loss/Loss_reward_critic        │ 258658144.0            │\n",
       "│ Loss/Loss_reward_critic/Delta  │ 258658144.0            │\n",
       "│ Value/reward                   │ -0.013693263754248619  │\n",
       "│ Loss/Loss_cost_critic          │ 487663360.0            │\n",
       "│ Loss/Loss_cost_critic/Delta    │ 487663360.0            │\n",
       "│ Value/cost                     │ 0.14834555983543396    │\n",
       "│ Time/Total                     │ 0.036490678787231445   │\n",
       "│ Time/Rollout                   │ 0.01964116096496582    │\n",
       "│ Time/Update                    │ 0.015769004821777344   │\n",
       "│ Time/Epoch                     │ 0.0354456901550293     │\n",
       "│ Time/FPS                       │ 282.12744140625        │\n",
       "│ Metrics/LagrangeMultiplier/Mea │ 0.03599999099969864    │\n",
       "│ Metrics/LagrangeMultiplier/Min │ 0.03599999099969864    │\n",
       "│ Metrics/LagrangeMultiplier/Max │ 0.03599999099969864    │\n",
       "│ Metrics/LagrangeMultiplier/Std │ 0.0                    │\n",
       "└────────────────────────────────┴────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mMetrics                       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mValue                 \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Metrics/EpRet                  │ 28129.7109375          │\n",
       "│ Metrics/EpCost                 │ 34804.609375           │\n",
       "│ Metrics/EpLen                  │ 5.0                    │\n",
       "│ Train/Epoch                    │ 0.0                    │\n",
       "│ Train/Entropy                  │ 1.4189385175704956     │\n",
       "│ Train/KL                       │ 0.0001909744751174003  │\n",
       "│ Train/StopIter                 │ 1.0                    │\n",
       "│ Train/PolicyRatio/Mean         │ 1.0                    │\n",
       "│ Train/PolicyRatio/Min          │ 1.0                    │\n",
       "│ Train/PolicyRatio/Max          │ 1.0                    │\n",
       "│ Train/PolicyRatio/Std          │ 0.0                    │\n",
       "│ Train/LR                       │ 0.00019999999494757503 │\n",
       "│ Train/PolicyStd                │ 1.0                    │\n",
       "│ TotalEnvSteps                  │ 10.0                   │\n",
       "│ Loss/Loss_pi                   │ -7.62939453125e-06     │\n",
       "│ Loss/Loss_pi/Delta             │ -7.62939453125e-06     │\n",
       "│ Value/Adv                      │ 1.1920928955078125e-07 │\n",
       "│ Loss/Loss_reward_critic        │ 258658144.0            │\n",
       "│ Loss/Loss_reward_critic/Delta  │ 258658144.0            │\n",
       "│ Value/reward                   │ -0.013693263754248619  │\n",
       "│ Loss/Loss_cost_critic          │ 487663360.0            │\n",
       "│ Loss/Loss_cost_critic/Delta    │ 487663360.0            │\n",
       "│ Value/cost                     │ 0.14834555983543396    │\n",
       "│ Time/Total                     │ 0.036490678787231445   │\n",
       "│ Time/Rollout                   │ 0.01964116096496582    │\n",
       "│ Time/Update                    │ 0.015769004821777344   │\n",
       "│ Time/Epoch                     │ 0.0354456901550293     │\n",
       "│ Time/FPS                       │ 282.12744140625        │\n",
       "│ Metrics/LagrangeMultiplier/Mea │ 0.03599999099969864    │\n",
       "│ Metrics/LagrangeMultiplier/Min │ 0.03599999099969864    │\n",
       "│ Metrics/LagrangeMultiplier/Max │ 0.03599999099969864    │\n",
       "│ Metrics/LagrangeMultiplier/Std │ 0.0                    │\n",
       "└────────────────────────────────┴────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">Warning: trajectory cut off when rollout by epoch at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.0</span><span style=\"color: #008000; text-decoration-color: #008000\"> steps.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mWarning: trajectory cut off when rollout by epoch at \u001b[0m\u001b[1;36m10.0\u001b[0m\u001b[32m steps.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Metrics                        </span>┃<span style=\"font-weight: bold\"> Value                  </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Metrics/EpRet                  │ 28129.7109375          │\n",
       "│ Metrics/EpCost                 │ 34804.609375           │\n",
       "│ Metrics/EpLen                  │ 5.0                    │\n",
       "│ Train/Epoch                    │ 1.0                    │\n",
       "│ Train/Entropy                  │ 1.4186384677886963     │\n",
       "│ Train/KL                       │ 0.00010796579590532929 │\n",
       "│ Train/StopIter                 │ 1.0                    │\n",
       "│ Train/PolicyRatio/Mean         │ 0.9999998807907104     │\n",
       "│ Train/PolicyRatio/Min          │ 0.9999998807907104     │\n",
       "│ Train/PolicyRatio/Max          │ 0.9999998807907104     │\n",
       "│ Train/PolicyRatio/Std          │ 0.0                    │\n",
       "│ Train/LR                       │ 9.999999747378752e-05  │\n",
       "│ Train/PolicyStd                │ 0.9997000694274902     │\n",
       "│ TotalEnvSteps                  │ 20.0                   │\n",
       "│ Loss/Loss_pi                   │ 0.0001464843808207661  │\n",
       "│ Loss/Loss_pi/Delta             │ 0.0001541137753520161  │\n",
       "│ Value/Adv                      │ -8.940696716308594e-08 │\n",
       "│ Loss/Loss_reward_critic        │ 947240256.0            │\n",
       "│ Loss/Loss_reward_critic/Delta  │ 688582112.0            │\n",
       "│ Value/reward                   │ 0.007712819613516331   │\n",
       "│ Loss/Loss_cost_critic          │ 615220288.0            │\n",
       "│ Loss/Loss_cost_critic/Delta    │ 127556928.0            │\n",
       "│ Value/cost                     │ 0.1568843275308609     │\n",
       "│ Time/Total                     │ 0.07556390762329102    │\n",
       "│ Time/Rollout                   │ 0.015428543090820312   │\n",
       "│ Time/Update                    │ 0.010138273239135742   │\n",
       "│ Time/Epoch                     │ 0.025595903396606445   │\n",
       "│ Time/FPS                       │ 390.69842529296875     │\n",
       "│ Metrics/LagrangeMultiplier/Mea │ 0.07099999487400055    │\n",
       "│ Metrics/LagrangeMultiplier/Min │ 0.07099999487400055    │\n",
       "│ Metrics/LagrangeMultiplier/Max │ 0.07099999487400055    │\n",
       "│ Metrics/LagrangeMultiplier/Std │ 0.0                    │\n",
       "└────────────────────────────────┴────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mMetrics                       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mValue                 \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Metrics/EpRet                  │ 28129.7109375          │\n",
       "│ Metrics/EpCost                 │ 34804.609375           │\n",
       "│ Metrics/EpLen                  │ 5.0                    │\n",
       "│ Train/Epoch                    │ 1.0                    │\n",
       "│ Train/Entropy                  │ 1.4186384677886963     │\n",
       "│ Train/KL                       │ 0.00010796579590532929 │\n",
       "│ Train/StopIter                 │ 1.0                    │\n",
       "│ Train/PolicyRatio/Mean         │ 0.9999998807907104     │\n",
       "│ Train/PolicyRatio/Min          │ 0.9999998807907104     │\n",
       "│ Train/PolicyRatio/Max          │ 0.9999998807907104     │\n",
       "│ Train/PolicyRatio/Std          │ 0.0                    │\n",
       "│ Train/LR                       │ 9.999999747378752e-05  │\n",
       "│ Train/PolicyStd                │ 0.9997000694274902     │\n",
       "│ TotalEnvSteps                  │ 20.0                   │\n",
       "│ Loss/Loss_pi                   │ 0.0001464843808207661  │\n",
       "│ Loss/Loss_pi/Delta             │ 0.0001541137753520161  │\n",
       "│ Value/Adv                      │ -8.940696716308594e-08 │\n",
       "│ Loss/Loss_reward_critic        │ 947240256.0            │\n",
       "│ Loss/Loss_reward_critic/Delta  │ 688582112.0            │\n",
       "│ Value/reward                   │ 0.007712819613516331   │\n",
       "│ Loss/Loss_cost_critic          │ 615220288.0            │\n",
       "│ Loss/Loss_cost_critic/Delta    │ 127556928.0            │\n",
       "│ Value/cost                     │ 0.1568843275308609     │\n",
       "│ Time/Total                     │ 0.07556390762329102    │\n",
       "│ Time/Rollout                   │ 0.015428543090820312   │\n",
       "│ Time/Update                    │ 0.010138273239135742   │\n",
       "│ Time/Epoch                     │ 0.025595903396606445   │\n",
       "│ Time/FPS                       │ 390.69842529296875     │\n",
       "│ Metrics/LagrangeMultiplier/Mea │ 0.07099999487400055    │\n",
       "│ Metrics/LagrangeMultiplier/Min │ 0.07099999487400055    │\n",
       "│ Metrics/LagrangeMultiplier/Max │ 0.07099999487400055    │\n",
       "│ Metrics/LagrangeMultiplier/Std │ 0.0                    │\n",
       "└────────────────────────────────┴────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">Warning: trajectory cut off when rollout by epoch at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.0</span><span style=\"color: #008000; text-decoration-color: #008000\"> steps.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mWarning: trajectory cut off when rollout by epoch at \u001b[0m\u001b[1;36m9.0\u001b[0m\u001b[32m steps.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Metrics                        </span>┃<span style=\"font-weight: bold\"> Value                  </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Metrics/EpRet                  │ 21468.029296875        │\n",
       "│ Metrics/EpCost                 │ 25004.017578125        │\n",
       "│ Metrics/EpLen                  │ 3.6666667461395264     │\n",
       "│ Train/Epoch                    │ 2.0                    │\n",
       "│ Train/Entropy                  │ 1.4184980392456055     │\n",
       "│ Train/KL                       │ 2.7003712602891028e-05 │\n",
       "│ Train/StopIter                 │ 1.0                    │\n",
       "│ Train/PolicyRatio/Mean         │ 1.0                    │\n",
       "│ Train/PolicyRatio/Min          │ 1.0                    │\n",
       "│ Train/PolicyRatio/Max          │ 1.0                    │\n",
       "│ Train/PolicyRatio/Std          │ 0.0                    │\n",
       "│ Train/LR                       │ 0.0                    │\n",
       "│ Train/PolicyStd                │ 0.9995596408843994     │\n",
       "│ TotalEnvSteps                  │ 30.0                   │\n",
       "│ Loss/Loss_pi                   │ 2.441406286379788e-05  │\n",
       "│ Loss/Loss_pi/Delta             │ -0.0001220703179569682 │\n",
       "│ Value/Adv                      │ 3.5762788286319847e-08 │\n",
       "│ Loss/Loss_reward_critic        │ 864482816.0            │\n",
       "│ Loss/Loss_reward_critic/Delta  │ -82757440.0            │\n",
       "│ Value/reward                   │ 0.020163696259260178   │\n",
       "│ Loss/Loss_cost_critic          │ 714492288.0            │\n",
       "│ Loss/Loss_cost_critic/Delta    │ 99272000.0             │\n",
       "│ Value/cost                     │ 0.09646058082580566    │\n",
       "│ Time/Total                     │ 0.11941838264465332    │\n",
       "│ Time/Rollout                   │ 0.01572561264038086    │\n",
       "│ Time/Update                    │ 0.010321855545043945   │\n",
       "│ Time/Epoch                     │ 0.02607893943786621    │\n",
       "│ Time/FPS                       │ 383.4546813964844      │\n",
       "│ Metrics/LagrangeMultiplier/Mea │ 0.10524892807006836    │\n",
       "│ Metrics/LagrangeMultiplier/Min │ 0.10524892807006836    │\n",
       "│ Metrics/LagrangeMultiplier/Max │ 0.10524892807006836    │\n",
       "│ Metrics/LagrangeMultiplier/Std │ 0.0                    │\n",
       "└────────────────────────────────┴────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mMetrics                       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mValue                 \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Metrics/EpRet                  │ 21468.029296875        │\n",
       "│ Metrics/EpCost                 │ 25004.017578125        │\n",
       "│ Metrics/EpLen                  │ 3.6666667461395264     │\n",
       "│ Train/Epoch                    │ 2.0                    │\n",
       "│ Train/Entropy                  │ 1.4184980392456055     │\n",
       "│ Train/KL                       │ 2.7003712602891028e-05 │\n",
       "│ Train/StopIter                 │ 1.0                    │\n",
       "│ Train/PolicyRatio/Mean         │ 1.0                    │\n",
       "│ Train/PolicyRatio/Min          │ 1.0                    │\n",
       "│ Train/PolicyRatio/Max          │ 1.0                    │\n",
       "│ Train/PolicyRatio/Std          │ 0.0                    │\n",
       "│ Train/LR                       │ 0.0                    │\n",
       "│ Train/PolicyStd                │ 0.9995596408843994     │\n",
       "│ TotalEnvSteps                  │ 30.0                   │\n",
       "│ Loss/Loss_pi                   │ 2.441406286379788e-05  │\n",
       "│ Loss/Loss_pi/Delta             │ -0.0001220703179569682 │\n",
       "│ Value/Adv                      │ 3.5762788286319847e-08 │\n",
       "│ Loss/Loss_reward_critic        │ 864482816.0            │\n",
       "│ Loss/Loss_reward_critic/Delta  │ -82757440.0            │\n",
       "│ Value/reward                   │ 0.020163696259260178   │\n",
       "│ Loss/Loss_cost_critic          │ 714492288.0            │\n",
       "│ Loss/Loss_cost_critic/Delta    │ 99272000.0             │\n",
       "│ Value/cost                     │ 0.09646058082580566    │\n",
       "│ Time/Total                     │ 0.11941838264465332    │\n",
       "│ Time/Rollout                   │ 0.01572561264038086    │\n",
       "│ Time/Update                    │ 0.010321855545043945   │\n",
       "│ Time/Epoch                     │ 0.02607893943786621    │\n",
       "│ Time/FPS                       │ 383.4546813964844      │\n",
       "│ Metrics/LagrangeMultiplier/Mea │ 0.10524892807006836    │\n",
       "│ Metrics/LagrangeMultiplier/Min │ 0.10524892807006836    │\n",
       "│ Metrics/LagrangeMultiplier/Max │ 0.10524892807006836    │\n",
       "│ Metrics/LagrangeMultiplier/Std │ 0.0                    │\n",
       "└────────────────────────────────┴────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(21468.029296875, 25004.017578125, 3.6666667461395264)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_cfgs = {\n",
    "    'train_cfgs': {\n",
    "        'total_steps': 30,\n",
    "    },\n",
    "    'algo_cfgs': {\n",
    "        'steps_per_epoch': 10,\n",
    "        'update_iters': 1,\n",
    "    },\n",
    "}\n",
    "agent = omnisafe.Agent('PPOLag', 'Example-v0', custom_cfgs=custom_cfgs)\n",
    "agent.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "干得不错！我们已经完成了这个定制化环境的嵌入和训练。接下来，我们将进一步研究如何为环境指定超参数。\n",
    "\n",
    "### 参数设定\n",
    "\n",
    "我们从一个新的示例环境出发，假设这个环境需要传入一个名为 `num_agents` 的参数。我们将展示如何不修改OmniSafe的代码来完成参数设定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NewExampleEnv has not been registered yet\n"
     ]
    }
   ],
   "source": [
    "@env_register\n",
    "@remove_from_register\n",
    "class NewExampleEnv(ExampleEnv):  # 创造一个新环境\n",
    "    _support_envs: ClassVar[list[str]] = ['NewExample-v0', 'NewExample-v1']\n",
    "    num_agents: ClassVar[int] = 1\n",
    "\n",
    "    def __init__(self, env_id: str, **kwargs) -> None:\n",
    "        super(NewExampleEnv, self).__init__(env_id, **kwargs)\n",
    "        self.num_agents = kwargs.get('num_agents', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时，`num_agents` 参数为预设值：`1`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_env = NewExampleEnv('NewExample-v0')\n",
    "new_env.num_agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们将展示如何通过 OmniSafe 的接口对该参数进行修改并训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PPOLag.yaml from /home/jiayi/omnisafe_zjy/omnisafe/utils/../configs/on-policy/PPOLag.yaml\n",
      "{'num_agents': 2}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Logging data to .</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">/runs/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">PPOLag-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">{NewExample-v0}</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">/seed-000-2024-03-26-23-04-50/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">progress.csv</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mLogging data to .\u001b[0m\u001b[1;35m/runs/\u001b[0m\u001b[1;95mPPOLag-\u001b[0m\u001b[1;36m{\u001b[0m\u001b[1;36mNewExample-v0\u001b[0m\u001b[1;36m}\u001b[0m\u001b[1;35m/seed-000-2024-03-26-23-04-50/\u001b[0m\u001b[1;95mprogress.csv\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Save with config in config.json</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mSave with config in config.json\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_cfgs.update({'env_cfgs': {'num_agents': 2}})\n",
    "agent = omnisafe.Agent('PPOLag', 'NewExample-v0', custom_cfgs=custom_cfgs)\n",
    "agent.agent._env._env.num_agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "非常好！我们将 `num_agents` 设置为了2。这表示我们在未修改代码的情形下成功实现了超参数设定。\n",
    "\n",
    "### 训练信息记录\n",
    "\n",
    "在运行训练代码时，你可能已经发现 OmniSafe 通过 `Logger` 记录了训练信息，例如：\n",
    "\n",
    "```bash\n",
    "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
    "┃ Metrics                        ┃ Value                   ┃\n",
    "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
    "│ Metrics/EpRet                  │ 2.046875                │\n",
    "│ Metrics/EpCost                 │ 2.89453125              │\n",
    "│ Metrics/EpLen                  │ 3.25                    │\n",
    "│ Train/Epoch                    │ 3.0                     │\n",
    "...\n",
    "```\n",
    "那么我们可否将环境之中的信息输出到日志中呢？答案是肯定的，而且这个过程同样不需要修改OmniSafe的代码。只需要完成两个接口：\n",
    "1. 在 `__init__` 函数中，将需要输出的信息添加到`self.env_spec_log`中。\n",
    "2. 实例化 `spec_log` 函数，记录所需的信息。\n",
    "\n",
    "**请注意：** 目前OmniSafe仅支持在每一个epoch结束时记录这些信息，而不支持在每一个step结束时记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@env_register\n",
    "@remove_from_register\n",
    "class NewExampleEnv(ExampleEnv):\n",
    "    _support_envs: ClassVar[list[str]] = ['NewExample-v0', 'NewExample-v1']\n",
    "\n",
    "    # 定义需要记录的信息\n",
    "    def __init__(self, env_id: str, **kwargs) -> None:\n",
    "        super(NewExampleEnv, self).__init__(env_id, **kwargs)\n",
    "        self.env_spec_log = {'Env/Success_counts': 0}\n",
    "\n",
    "    # 通过step函数，更新环境信息\n",
    "    def step(self, action):\n",
    "        obs, reward, cost, terminated, truncated, info = super().step(action)\n",
    "        success = int(reward > cost)\n",
    "        self.env_spec_log['Env/Success_counts'] += success\n",
    "        return obs, reward, cost, terminated, truncated, info\n",
    "\n",
    "    # 在logger中记录环境信息\n",
    "    def spec_log(self, logger) -> dict[str, Any]:\n",
    "        logger.store({'Env/Success_counts': self.env_spec_log['Env/Success_counts']})\n",
    "        self.env_spec_log['Env/Success_counts'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们简单训练观察该信息是否被成功记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PPOLag.yaml from /home/jiayi/omnisafe_zjy/omnisafe/utils/../configs/on-policy/PPOLag.yaml\n",
      "{'num_agents': 2}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Logging data to .</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">/runs/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">PPOLag-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">{NewExample-v0}</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">/seed-000-2024-03-26-23-04-59/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">progress.csv</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36mLogging data to .\u001b[0m\u001b[1;35m/runs/\u001b[0m\u001b[1;95mPPOLag-\u001b[0m\u001b[1;36m{\u001b[0m\u001b[1;36mNewExample-v0\u001b[0m\u001b[1;36m}\u001b[0m\u001b[1;35m/seed-000-2024-03-26-23-04-59/\u001b[0m\u001b[1;95mprogress.csv\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Save with config in config.json</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mSave with config in config.json\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">INFO: Start training</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mINFO: Start training\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Metrics                        </span>┃<span style=\"font-weight: bold\"> Value                  </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Metrics/EpRet                  │ 28129.7109375          │\n",
       "│ Metrics/EpCost                 │ 34804.609375           │\n",
       "│ Metrics/EpLen                  │ 5.0                    │\n",
       "│ Train/Epoch                    │ 0.0                    │\n",
       "│ Train/Entropy                  │ 1.4189385175704956     │\n",
       "│ Train/KL                       │ 0.00019391297246329486 │\n",
       "│ Train/StopIter                 │ 1.0                    │\n",
       "│ Train/PolicyRatio/Mean         │ 1.0                    │\n",
       "│ Train/PolicyRatio/Min          │ 1.0                    │\n",
       "│ Train/PolicyRatio/Max          │ 1.0                    │\n",
       "│ Train/PolicyRatio/Std          │ 0.0                    │\n",
       "│ Train/LR                       │ 0.0                    │\n",
       "│ Train/PolicyStd                │ 1.0                    │\n",
       "│ TotalEnvSteps                  │ 10.0                   │\n",
       "│ Loss/Loss_pi                   │ 5.035400317865424e-05  │\n",
       "│ Loss/Loss_pi/Delta             │ 5.035400317865424e-05  │\n",
       "│ Value/Adv                      │ -6.854534007061375e-08 │\n",
       "│ Loss/Loss_reward_critic        │ 258657840.0            │\n",
       "│ Loss/Loss_reward_critic/Delta  │ 258657840.0            │\n",
       "│ Value/reward                   │ -0.010763613507151604  │\n",
       "│ Loss/Loss_cost_critic          │ 487662016.0            │\n",
       "│ Loss/Loss_cost_critic/Delta    │ 487662016.0            │\n",
       "│ Value/cost                     │ 0.197834774851799      │\n",
       "│ Time/Total                     │ 0.04369854927062988    │\n",
       "│ Time/Rollout                   │ 0.0246431827545166     │\n",
       "│ Time/Update                    │ 0.017728805541992188   │\n",
       "│ Time/Epoch                     │ 0.04240918159484863    │\n",
       "│ Time/FPS                       │ 235.80064392089844     │\n",
       "│ Env/Success_counts             │ 1.5                    │\n",
       "│ Metrics/LagrangeMultiplier/Mea │ 0.03599999099969864    │\n",
       "│ Metrics/LagrangeMultiplier/Min │ 0.03599999099969864    │\n",
       "│ Metrics/LagrangeMultiplier/Max │ 0.03599999099969864    │\n",
       "│ Metrics/LagrangeMultiplier/Std │ 0.0                    │\n",
       "└────────────────────────────────┴────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mMetrics                       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mValue                 \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Metrics/EpRet                  │ 28129.7109375          │\n",
       "│ Metrics/EpCost                 │ 34804.609375           │\n",
       "│ Metrics/EpLen                  │ 5.0                    │\n",
       "│ Train/Epoch                    │ 0.0                    │\n",
       "│ Train/Entropy                  │ 1.4189385175704956     │\n",
       "│ Train/KL                       │ 0.00019391297246329486 │\n",
       "│ Train/StopIter                 │ 1.0                    │\n",
       "│ Train/PolicyRatio/Mean         │ 1.0                    │\n",
       "│ Train/PolicyRatio/Min          │ 1.0                    │\n",
       "│ Train/PolicyRatio/Max          │ 1.0                    │\n",
       "│ Train/PolicyRatio/Std          │ 0.0                    │\n",
       "│ Train/LR                       │ 0.0                    │\n",
       "│ Train/PolicyStd                │ 1.0                    │\n",
       "│ TotalEnvSteps                  │ 10.0                   │\n",
       "│ Loss/Loss_pi                   │ 5.035400317865424e-05  │\n",
       "│ Loss/Loss_pi/Delta             │ 5.035400317865424e-05  │\n",
       "│ Value/Adv                      │ -6.854534007061375e-08 │\n",
       "│ Loss/Loss_reward_critic        │ 258657840.0            │\n",
       "│ Loss/Loss_reward_critic/Delta  │ 258657840.0            │\n",
       "│ Value/reward                   │ -0.010763613507151604  │\n",
       "│ Loss/Loss_cost_critic          │ 487662016.0            │\n",
       "│ Loss/Loss_cost_critic/Delta    │ 487662016.0            │\n",
       "│ Value/cost                     │ 0.197834774851799      │\n",
       "│ Time/Total                     │ 0.04369854927062988    │\n",
       "│ Time/Rollout                   │ 0.0246431827545166     │\n",
       "│ Time/Update                    │ 0.017728805541992188   │\n",
       "│ Time/Epoch                     │ 0.04240918159484863    │\n",
       "│ Time/FPS                       │ 235.80064392089844     │\n",
       "│ Env/Success_counts             │ 1.5                    │\n",
       "│ Metrics/LagrangeMultiplier/Mea │ 0.03599999099969864    │\n",
       "│ Metrics/LagrangeMultiplier/Min │ 0.03599999099969864    │\n",
       "│ Metrics/LagrangeMultiplier/Max │ 0.03599999099969864    │\n",
       "│ Metrics/LagrangeMultiplier/Std │ 0.0                    │\n",
       "└────────────────────────────────┴────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(28129.7109375, 34804.609375, 5.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_cfgs.update({'train_cfgs': {'total_steps': 10}})\n",
    "agent = omnisafe.Agent('PPOLag', 'NewExample-v0', custom_cfgs=custom_cfgs)\n",
    "agent.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "漂亮！上述代码将在终端输出了环境特化的信息 `Env/Success_counts`。这一过程并不需要对原代码作出改动。\n",
    "\n",
    "## 总结\n",
    "安全强化学习是为了AI朝着更鲁棒与安全方向发展的重要研究方向。OmniSafe希望能够成为安全强化学习的有用开源工具包，为AI安全研究提供良好的基础。我们将持续完善OmniSafe的环境定制化接口，使OmniSafe能够更好的适应各种安全强化学习任务。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnisafe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

# Copyright 2022-2023 OmniSafe Team. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

defaults:
  # --------------------------------------Basic Configurations----------------------------------- #
  ## -----------------------------Basic configurations for base class DDPG---------------------- ##
  # The random seed
  seed: 0
  # The number of threads used per experiment
  num_threads: 1
  # If use tensorboard
  use_tensorboard: True
  # if use wandb
  use_wandb: False
  # The torch device
  device: cpu
  # The torch device id
  device_id: 0
  # The environment wrapper type
  wrapper_type: CMDPWrapper
  # Number of total steps
  total_steps: 1.0e+6
  # Number of steps per epoch
  steps_per_epoch: 2000
  # Number of steps per sample
  steps_per_sample: 1
  # Number of random steps
  random_steps: 5000
  # Update every `update_every` steps
  update_cycle: 1
  # Check if all models own the same parameter values every `check_freq` epochs
  check_freq: 25
  # Save model to disk every `check_freq` epochs
  save_freq: 100
  # The max length of per epoch
  max_ep_len: 1000
  # The learning rate of Actor network
  actor_lr: 0.0001
  # The learning rate of Critic network
  critic_lr: 0.0003
  # The number of parallel environments
  num_envs: 2
  # Whether to use standardized reward
  reward_normalize: False
  # Whether to use standardized cost
  cost_normalize: False
  # Whether to use standardized obs
  obs_normalize: False
  # The soft update coefficient
  polyak: 0.005
  # The discount factor of GAE
  gamma: 0.99
  # Actor perdorm random action before `start_steps` steps
  start_learning_steps: 5000
  # The Address for saving training process data
  data_dir: "./runs"
  # The number of episode to test
  num_test_episodes: 10
  # The delay step of policy update
  policy_delay: 2
  # Whether to use the exploration noise
  use_exploration_noise: False
  # The exploration noise
  exploration_noise: 0.0
  # The value of alpha
  alpha: 0.2
  # Whether to use auto alpha
  auto_alpha: False

  # ---------------------------------------Optional Configuration-------------------------------- #
  ## -----------------------------------Configuration For Cost Critic--------------------------- ##
  # Whether to use cost critic
  use_cost: True
  # Whether to use cost limit decay
  cost_limit_decay: False
  # The initial value of cost limit
  init_cost_limit: 25.0
  # The target value of cost limit
  target_cost_limit: 25.0
  # The end of cost limit decay epoch
  end_epoch: 100
  # Cost discounted factor
  cost_gamma: 1.0
  # Whether to use linear decay of learning rate
  linear_lr_decay: False
  # Whether to use exploration noise anneal
  exploration_noise_anneal: False
  # Whether to use reward penalty
  reward_penalty: False
  # Whether to use max gradient norm
  use_max_grad_norm: False
  # The thereshold of max gradient norm
  max_grad_norm: 40
  # Whether to use critic network norm
  use_critic_norm: False
  # The norm coefficient of critic network
  critic_norm_coeff: 0.001
  ## ---------------------------------------Configuration For Model----------------------------- ##
  model_cfgs:
    # The mode to initiate the weight of network, choosing from "kaiming_uniform", "xavier_normal", "glorot" and "orthogonal".
    weight_initialization_mode: "kaiming_uniform"
    # Type of Actor, choosing from "gaussian_annealing", "gaussian_std_net_actor", "gaussian_learning_actor", "categorical_actor"
    actor_type: gaussian_sac
    # Whether to use linear decay of learning rate
    linear_lr_decay: True
    # Configuration of Actor network
    actor:
      # Size of hidden layers
      hidden_sizes: [256, 256]
      # Activation function
      activation: relu
      # The learning rate of Actor network
      lr: 0.0003
    # Configuration of Critic network
    critic:
      # The number of critic networks
      num_critics: 2
      # Size of hidden layers
      hidden_sizes: [256, 256]
      # Activation function
      activation: relu
      # The learning rate of Critic network
      lr: 0.0003
  ## --------------------------------------Configuration For Buffer----------------------------- ##
  replay_buffer_cfgs:
    # The size of replay buffer
    size: 100000
    # The size of batch
    batch_size: 256
  ## --------------------------------------Configuration For Environment------------------------ ##
  env_cfgs:
    # The number of parallel environments
    num_envs: 1
    # Whether to use async environment
    async_env: True
    # Whether to use standardized reward
    normalized_rew: False
    # Whether to use standardized cost
    normalized_cost: False
    # Whether to use standardized obs
    normalized_obs: True
    # The maximum length of record queue
    max_len: 100
    # The number of threads used to sample data
    num_threads: 20
  ## --------------------------------------Configuration For Lagrange--------------------------- ##
  lagrange_cfgs:
    # Tolerance of constraint violation
    cost_limit: 25.0
    # Initial value of lagrangian multiplier
    lagrangian_multiplier_init: 0.000
    # Learning rate of lagrangian multiplier
    lambda_lr: 0.07
    # Type of lagrangian optimizer
    lambda_optimizer: "Adam"
